{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "get = requests.get('https://www.gutenberg.org/files/100/100-0.txt')\n",
    "\n",
    "text = get.text.strip()\n",
    "chars = sorted(list(set(text)))\n",
    "chars_indicies = dict((c,i) for i, c in enumerate(chars))\n",
    "indicies_char = dict((i,c) for i,c in enumerate(chars))\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "\n",
    "sentences = [] # x\n",
    "next_chars = [] # y \n",
    "\n",
    "for i in range(0, 100000 - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i,t,chars_indicies[char]] = 1\n",
    "    y[i, chars_indicies[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               124416    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 114)               14706     \n",
      "=================================================================\n",
      "Total params: 139,122\n",
      "Trainable params: 139,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, chars_indicies[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indicies_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "33280/33320 [============================>.] - ETA: 0s - loss: 2.4558\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"out.\n",
      "\n",
      "        During this song they pi\"\n",
      "out.\n",
      "\n",
      "        During this song they pise thes seat mes mese,\n",
      "And see thes thes seate thes sees seate,\n",
      "The seres leat thes thes mes love thes seate,\n",
      "And thes thes thes mest thes stall thes leat thes best mes seate,\n",
      "And thes love thes thes thes thes thes seat thes mes seate,\n",
      "And thes beat thes thes love thes and thes thes nes in sere,\n",
      "And thes best thes stall sees mess thes thes thes thes mest thes ane thes stere,\n",
      "And thes thes t\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"out.\n",
      "\n",
      "        During this song they pi\"\n",
      "out.\n",
      "\n",
      "        During this song they pill ne of my thas deres thes,\n",
      "s thes stere thes best and ane leare thes gall,\n",
      "And sall thes whes men this mates allacs gees\n",
      "Thes lest leser thy seate seees bean I start nes meen thes menes thes bease,\n",
      "And that dest lee hose thes beall thes mend,\n",
      "And fall laste thes mast that thes sone,\n",
      "Whece thes list the  lost and thes nes thes thes thes all thes llat steres seace,\n",
      "Thes seate thes thes mess\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"out.\n",
      "\n",
      "        During this song they pi\"\n",
      "out.\n",
      "\n",
      "        During this song they piesen tret,\n",
      "So frert beprellito teith d senylgam.\n",
      "\n",
      "\n",
      "                   O  nT THR\n",
      "ASflre y y thou chat orass peathesluglind nes de,\n",
      "Ath tris it weallceee thy sthan, meile :\n",
      "Ang anprowtone this mes than ceeant, an elroinal oftart:\n",
      "  leve I illesess deathey ancemcy thanier,\n",
      "Manle thir yos salest ren veslidilpast hes deall.\n",
      "\n",
      "\n",
      "                    WHII S vedIn tost Iis tnes dis eres hes yos w\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"out.\n",
      "\n",
      "        During this song they pi\"\n",
      "out.\n",
      "\n",
      "        During this song they piliteste oo emmes baglgsd faene,\n",
      "Aot llat geor Thesswheanfeess care thee nowim tils,\n",
      "Then urealestgh welludewkacatsevinllthase gresine,\n",
      "Tor stall kesueiwe  yerse, jrtict becce bunsptdange.\n",
      "Sedebwlighan, deay.\n",
      "Thet aallr padec, to costhf one mOrsseny,\n",
      "Irkell utctyed, tars sfermssath tradocergrde that surer,\n",
      "Tu timoles fitbed bearlasts les yfrasteve time  hod,\n",
      "es whort-hen butithus her floalt\n",
      "33320/33320 [==============================] - 182s 5ms/sample - loss: 2.4551\n",
      "Epoch 2/3\n",
      "33280/33320 [============================>.] - ETA: 0s - loss: 1.8801\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"buses;\n",
      "Love thrives not in the heart th\"\n",
      "buses;\n",
      "Love thrives not in the heart the sere,\n",
      "The ere the with shall and the with the sere,\n",
      "The ere the wert me shath the with the were,\n",
      "The  he for the sere shath the here sears,\n",
      "The  here the with where here where shere,\n",
      "The ser wher be and the with she fare,\n",
      "The  her where and the sere in the werte,\n",
      "The sere and for me the wrow the werte,\n",
      "The ere the wert me shath the wert where,\n",
      "And the with where the sere me shath the we\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"buses;\n",
      "Love thrives not in the heart th\"\n",
      "buses;\n",
      "Love thrives not in the heart that where.\n",
      "\n",
      "\n",
      "                                                                                                                                                                  1                                                                                                                                                                                                                              \n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"buses;\n",
      "Love thrives not in the heart th\"\n",
      "buses;\n",
      "Love thrives not in the heart the wirh se beass,\n",
      "Tisw whang in that dharond shalg have his,\n",
      "Sham bat ulon.\n",
      "\n",
      "\n",
      "                          D 33\n",
      "\n",
      "For hocth ffos me noe of st lied of ever,\n",
      "  O mank that hour in ill sherlire af on ule,\n",
      "Uave forr pefestutg prome chets winkent,\n",
      "Thy war eNortinge ichat ow at feareine,\n",
      "And bars rers vor sour of lite,\n",
      "For she seais be bnood sal co seaven\n",
      "What lute wrees serre-as co wret me thin\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"buses;\n",
      "Love thrives not in the heart th\"\n",
      "buses;\n",
      "Love thrives not in the heart the rebercs\n",
      "Bet hank that thLther selougser herent,\n",
      "I his min thassrseeâ amh far semank shere,\n",
      "Whou, I âut wuull to kergioua agiive inoun,\n",
      "Ros where as comather , shatk micowbit de.\n",
      "  And forloâu wherererere the tomy net,\n",
      "Sorruge nolh she and that othear her,\n",
      "Thosed extrurwad, an kee fut, ghatins ee,\n",
      "Thy brected perch depuered..\n",
      "That wlail I sherriuch where erirje forius Rios diil \n",
      "\n",
      "33320/33320 [==============================] - 191s 6ms/sample - loss: 1.8801\n",
      "Epoch 3/3\n",
      "33280/33320 [============================>.] - ETA: 0s - loss: 1.7132\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "\"---- Generating with seed: \"n thee therefore with a flood of tears,\n",
      "n thee therefore with a flood of tears,\n",
      "  And the seare the reare then then for me to thee,\n",
      "And to to thee thee for me to me thee thee to to thee,\n",
      "  And thee the seare would thee to me to thee thee to me to me to me.\n",
      "\n",
      "\n",
      "                                                                                                                                                                                                                       \n",
      "----- diversity: 0.5\n",
      "\"---- Generating with seed: \"n thee therefore with a flood of tears,\n",
      "n thee therefore with a flood of tears,\n",
      "And his with the would thy see thou de thee I and allove,\n",
      "  Of the tay here to this for sue their sue,\n",
      "  My to me thee with that for thou doth stree,\n",
      "  And me to to cond at thee for me to condens,\n",
      "And to have the were so now thee with not I mend,\n",
      "  My vere reco deatering thee in so doth time,\n",
      "  To herefore be of the reared outen to me\n",
      "  And I conders of the for thou me then and be.\n",
      "\n",
      "\n",
      "  \n",
      "----- diversity: 1.0\n",
      "\"---- Generating with seed: \"n thee therefore with a flood of tears,\n",
      "n thee therefore with a flood of tears,\n",
      "  The sealliks time nestes forther prore bene ame,\n",
      "The lewelen fie line of of oomet kith tile,\n",
      "  Of to I hus grace olecior, af tiee thee,\n",
      "Buted but bust thou dein lend worn kind,\n",
      "O Save to stame thou doth from love on finfature,\n",
      "The fale I knot atton distround love upSenctiis,\n",
      "Wher beautune herutiou of buce id to teat deasudes\n",
      "Mether bake of be deace all 2pren re,\n",
      "O hate beast withor peit\n",
      "----- diversity: 1.2\n",
      "\"---- Generating with seed: \"n thee therefore with a flood of tears,\n",
      "n thee therefore with a flood of tears,\n",
      "So him me of of thiny my ferdmorer,\n",
      "Geceence bunnser frob my who so at hapine rispamined seach:âs my tead yever kinis.\n",
      "  Hot hather deamy toor your I and thee of,\n",
      "Ando\n",
      "\n",
      "Sand wour fararaurâs cuefey ay youn pe.\n",
      "  Butuyou sourst, deawids pruluget to lic txen,\n",
      "Cive wathnd.\n",
      " bery hour, you  heave a say as the sheurw,\n",
      "Factlint and wint headel- so gids ventect.\n",
      "eyharoyaus more sole deept, \n",
      "33320/33320 [==============================] - 187s 6ms/sample - loss: 1.7129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63c9f3978>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=3,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
